{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Full Archive Search API v2\n",
    "This Jupyter notebook allows you to use the full archive search endpoint for Twitter's API v2.\n",
    "You must be approved for the Academic Research track to use this script.\n",
    "\n",
    "The last cell of the notebook allows you to specify the parameters for your search. You will need a bearer token to authenticate.\n",
    "\n",
    "<b>NOTE:</b>\n",
    "- The endpoint allows you to make only 1 request per second and 300 requests per 15 minutes. Accordingly, the script will pause for 3.1 seconds between each request to stay within the rate limits.\n",
    "- The search endpoint returns Tweet objects. The v2 API has significantly revamped the object model, so extended fields will need to be specified separately using the new fields and expansions parameters. Although additional information about the user are available for each tweet, the expansions parameter is slightly tricky to deal with. I'm currently working on a solution to add more user detail fields apart from the user id. If you're feeling adventurous, [you can refer to the documentation for fields and expansions](https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet) to modify the code below!\n",
    "\n",
    "#### Execute the next three cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_endpoint_connect(bearer_token, query, st, et, next_token):\n",
    "    \n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    query_params = {\n",
    "                    'query': query,\n",
    "                    'start_time': st,\n",
    "                    'end_time': et,\n",
    "                    'max_results': 500,\n",
    "                    'tweet.fields': 'id,text,author_id,created_at,geo,lang,public_metrics,in_reply_to_user_id',               \n",
    "                   }\n",
    "    \n",
    "    if (next_token is not None):\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all?next_token={}\".format(next_token)\n",
    "    else:\n",
    "        url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    \n",
    "    response = requests.request(\"GET\", url, params=query_params, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(bearer_token, n, fn, sq, st, et):\n",
    "    \n",
    "    count = 0\n",
    "    flag = True\n",
    "    first = True\n",
    "\n",
    "\n",
    "    while flag:\n",
    "        \n",
    "\n",
    "        if count >= n and n!=0:\n",
    "            break\n",
    "        if not first:\n",
    "            json_response = search_endpoint_connect(bearer_token, sq, st, et, next_token)\n",
    "        else:\n",
    "            json_response = search_endpoint_connect(bearer_token, sq, st, et, next_token=None)\n",
    "        \n",
    "        result_count = json_response['meta']['result_count']\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "        \n",
    "        if result_count is not None and result_count > 0:\n",
    "            \n",
    "            df = pd.json_normalize(json_response['data'])\n",
    "            df = df.reindex(columns=['id', 'author_id', 'created_at', 'text', 'lang', 'public_metrics.retweet_count',\n",
    "                                     'public_metrics.reply_count', 'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                                    'in_reply_to_user_id', 'geo.place_id', 'geo.coordinates.type', 'geo.coordinates.coordinates'])\n",
    "            if not first:\n",
    "                df.to_csv('%s.csv'%fn, mode='a', encoding='utf-8', index=False, header=None)\n",
    "            else:\n",
    "                df.to_csv('%s.csv'%fn, encoding='utf-8', index=False)\n",
    "\n",
    "        time.sleep(3.1)\n",
    "        count += result_count\n",
    "        print('Tweets downloaded: '+str(count))\n",
    "        if 'next_token' not in json_response['meta']:\n",
    "            flag = False\n",
    "        first = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting your bearer token and search parameters\n",
    "\n",
    "- You should have access to Twitter's Academic Research Product Track to obtain the bearer token.\n",
    "- You can limit the number of tweets returned for the query by setting a desired value. Note that the minimum will be somewhere in the vicinity of 500 since that is maximum number of results Twitter returns <b>per request.</b> Setting it to 0 will return the full set of results. <b> Be careful about your search parameters as this can easily exhaust your monthly quota of 10 million tweets.</b>\n",
    "- The output will be written to a csv file. You can change the file name down below. <b>Do not include .csv in the file name here.</b>\n",
    "- You can craft your search query using a variety of different parameters. You can find a list of [supported search query parameters here](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "- For the start and end time, you should follow the YYYY-MM-DDTHH:MM:SSZ format. Note that the date and time should be separated by a <b>capital T</b>. The <b>capital Z</b> after the time allows for specifiying an offset from UTC, which is the default. <b>Example:</b> 2019-11-27T00:00:00Z to specify midnight of 27th November 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your bearer token\n",
    "bearer_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "#Set number of tweets to be downloaded. Enter 0 for no limits\n",
    "no_of_tweets = 20\n",
    "\n",
    "#Specify the name of the output csv file. Do not include .csv\n",
    "file_name = 'downloaded_tweets'\n",
    "\n",
    "#Enter your search query. Refer to https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query\n",
    "search_query = '#your #query #here'\n",
    "\n",
    "#Set the beginning date and time in YYYY-MM-DDTHH:MM:SSZ format\n",
    "start_time = \"YYYY-MM-DDTHH:MM:SSZ\"\n",
    "\n",
    "#Set the ending date and time in YYYY-MM-DDTHH:MM:SSZ format\n",
    "end_time = \"YYYY-MM-DDTHH:MM:SSZ\"\n",
    "\n",
    "main(bearer_token, no_of_tweets, file_name, search_query, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
